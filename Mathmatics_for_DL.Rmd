---
title: "Mathmatics for Deep learning"
output: 
  html_document :
    toc : true
    toc_float: true
header-includes:
    - \usepackage{kotex}
urlcolor : blue
editor_options: 
  chunk_output_type: inline
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 소개

이 파일을 통해 딥러닝과 NLP에 필요한 기본 개념과 여러 모델들을 간단하게나마 수학적으로 풀어나가보고자 한다.

# 주성분분석, 공분산행렬, 고윳값과 고유벡터

## `주성분분석 (PCA)`이 무엇인가요?

주성분분석 (*Principal component analysis*)    
→ 원래 데이터들을 그대로 사용하는 것이 아니라, 원래 데이터들의 `Principal Component` 를 가지고 분석하는 것입니다.    
어려운 말로 쓰자면, 분석에 포함된 변수들의 선형결합을 이용하여 주성분이라 불리는 새로운 변수를 생성하는 방법입니다.    
p개의 변수에는 p개의 주성분이 존재하고, 이 중에 중요한 주성분을 찾아서 그 의미를 해석하게 됩니다.    
변수들의 선형결합으로 이루어진 주성분을 이용하기에 자료가 갖고 있는 대부분의 정보를 활용할 수 있다는 장점이 있습니다.    
예를들어 고등학교에는 여러 교과목이 있지만, 이를 가중평균을 내어 수리영역, 언어영역으로 나누는 것들이 예시입니다.
    
    
- - -


## 그렇다면 `주성분`은 무엇인가요?

Principal Component, 주성분이란 `주어진 데이터(의 분산)을 가장 잘 표현하는 성분`입니다.
```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '100%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i1.gif")
```
    
    
- - -

## 주성분은 `어떻게` 구해요?

주성분은 아무렇게나 구해도 `이론적으론` 상관 없습니다.    

그래도 우리는 데이터 공부하는 사람이니,

* 주어진 데이터의 `정보를 최대한 잃지 않게`
* 주어진 데이터의 `분산을 가장 잘 설명할 수 있게  `  
* 주어진 데이터의 `중심을 지날 수 있게   ` 

그래도 기왕이면 위의 방법들을 거치는 것이 좋지 않을까요?
    
    
- - -

## 그 방법들을 만족하는 주성분을 구하는 `방법`이 있나요?

우선 그 방법을 알기 위해선 간단한 수학 개념을 짚고 가야 합니다.    

* **표준화** (주어진 데이터의 중심을 지날 수 있게, 혹은 단위의 영향이 클까봐(e.g. cm, kg))
* **분산공분산행렬** (주어진 데이터를 가장 잘 설명할 수 있는 하나의 행렬)    
* 그 행렬의 **고윳값과 고유벡터** (주어진 데이터의 분산을 가장 잘 설명할 수 있게)

이 방법들을 만족하는 수학적 방법들을 살펴보겠습니다.
    
    
- - -

### 표준화, 주어진 데이터의 중심을 지나보자.

표준화란, 주어진 데이터의 속성(feature)들을 평균이 0, 분산이 1인 데이터로 변환해주는 과정을 말합니다. 이 과정을 통해야 저희는 모든 속성들을 같은 scale로 볼 수가 있습니다.

$$
z = \frac{x - \mu}{\sigma}
$$
때로는 표준편차로 나눠주지 않고, 평균만 빼주는 경우도 있습니다.



- - -       

### 분산공분산행렬, 주어진 데이터들을 가장 잘 설명하는 행렬

**분산공분산행렬**이란,    
일종의 행렬로써, 데이터의 구조를 설명해주며, 특히 특징 쌍(feature pairs)들의 변동이 얼마나 닮았는가(다른 말로는 얼마만큼이나 함께 변하는가)를 행렬에 나타내고 있습니다.    
보통 기호로는 이렇게 나타냅니다.
$$
\sum
$$

행렬은 **사상(mapping)**입니다.    
쉽게 말하면, 어느 임의의 벡터에 행렬을 곱하면 그 벡터가 바뀐다는 의미입니다.    
기존의 어떤 데이터가 있었다고 하면 우리는 그 **데이터가 어떤 행렬의 곱으로 인해 바뀐 데이터**라고 생각해봅시다.

```{r, echo=F, out.width='50%', fig.ncol=2, fig.align='center'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i3.png")
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i2.png")
```

이번엔 공분산행렬의 값들이 나타내는 건 무엇인지 알아봅시다.
```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '100%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i4.png")
```
```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '100%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i14.png")
```


그렇다면 공분산행렬은 어떻게 구할 수 있을까요?

```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '100%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i10.gif")
```

    
       
- - -

### 그래서요?

결론부터 말씀드리자면, 우리는 데이터의 공분산행렬의 고윳값과 고유벡터를 구할 수만 있다면 데이터의 주성분을 바로 구할 수 있습니다.


- - - 

### 고윳값과 고유벡터는 어떻게 구하는데요?

  
선형대수를 알고계신다면, 고윳값과 고유벡터 관련해서 크기는 변해도 방향은 변하지 않는 값이며, 수식으로는

$$
Ax = \lambda x
$$
로 나타낼 수 있습니다. 이를 만족하는 람다 값이 고윳값이며, 이 람다를 통해서 고유벡터도 구할 수 있습니다.    
즉 어떤 벡터 x에 행렬 A를 곱하고 나서 보니, 그냥 벡터 x에 어떤 상수값 람다를 곱한거나 다름이 없었다는거죠.    
A를 통해서 mapping된 Ax는 사실 x에 상수(람다)만 곱한거와 같으니, 사실은 방향의 변환이 없이 크기만 커진 것과 다름이 없는 겁니다.
    
예시를 들어 설명 드리겠습니다. 다음과 같은 행렬 A, X가 있다고 가정해보죠.  
```{r}
A = matrix(c(2,1,1,2), byrow = T, ncol=2);A
X = matrix(c(1,1), ncol=1);X
```


벡터 X를 좌표에 나타내면 다음과 같습니다.
```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '50%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i11.png")
```

근데 이 벡터 x에 행렬 A를 곱해주면,

```{r}
A%*%X
```

이런 값이 나옵니다. 이 벡터(AX)를 다시 한 번 좌표축에 나타내보면,

```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '50%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i12.png")
```

이렇게 표시할 수 있죠.    

기존의 빨간 (1,1) 벡터에서 (3,3) 벡터로 늘어난 것을 확인할 수 있습니다.     
이럴 거였으면, 벡터 X에 행렬 A를 곱할 필요 없이 그냥 3을 곱해버렸으면 되는 것 아닐까요?    

### 맞습니다.

그게 바로 고윳값의 의미입니다. 다시 한 번 수식과 함께 `고유`라는 것의 의미를 곱씹어보면,
$$
Ax = \lambda x
$$

크기만 변했고 방향은 변하지 않았죠. 이 크기 3을 우리는 고윳값이라고 하며, 이 방향을 고유벡터라고 얘기합니다. 손으로 구하는 방법은 특성방정식과 행렬식에 대해서 알아보시길 권장합니다.     

이름도 어려운 특성방정식과 행렬식을 이용해서 사실 조금 더 엄밀하게 계산해보면 고윳값은 3과 1이 나옵니다. 그리고 이를 통해서 고유벡터도 구해보면,

```{r}
eigen(A)
```

여기서 `$vectors`를 보시면 0.7 비스무리한 숫자로 나와있지만, 엄밀하게 얘기해보면
그냥 (1,1)과 (-1,1)로 생각해도 무관합니다. 



공분산 행렬이 A라고 해봅시다.   
그랬을 때 정리해보면,    
우리는 어떤 데이터를 어떤 행렬을 곱해서 사상된 데이터로 생각해보기로 했습니다. 공분산행렬을 통해 선형변환을 했을 때 그 크기는 변하고 방향이 변하지 않는 벡터가 있다고 하면, 그 벡터의 크기는 각 1배와 3배가 된다는 의미입니다. 그리고 그 방향성은 (1,1)과 (-1,1)이라는거죠.


조금 더 어려운 점은, 이 Matrix의 고유값은 1과 3임을 생각해보면서 따져보면,   

* 이 고윳값의 합은 1 + 3 = 4로, 각 피쳐의 분산의 합과 같습니다.    
* 이 고윳값의 곱은 Matrix의 행렬식과 같습니다.

즉 이 분산공분산행렬과, 고윳값과 고유벡터만 있으면    

* 그 행렬의 고유의 방향성(고유벡터)
* 그 행렬의 고유의 방향성의 크기(고윳값)
* 각 feature의 분산(행렬의 대각원소)
* 각 feature끼리 얼마나 변하는지(행렬의 비대각원소)


까지 다 알 수 있다는 것이죠.


### 아니 그래서요?

주성분분석으로 돌아와보겠습니다. 우리는 그래서 주성분을 어떻게 고를거냐면,

* 분산이 가장 커지는 축을 첫번째 주성분으로 하고,
* 분산이 두번째로 커지는 축을 두번째 주성분으로 하고,
* 분산이 세번째로 커지는 축을 세번째 주성분으로 합니다.
* 조금 더 어려운 말은 각 주성분은 서로간 직교합니다. (공분산행렬의 고유벡터라서..)

이런 식으로 주성분을 선정한다면 우리는 원 데이터를 가장 잘 설명하는 주성분을 선택할 수 있다는겁니다. 조금 어려운 말로 풀어보자면 주성분 분석은 가장 큰 분산을 갖는 부분공간을 보존하는 최적의 선형변환을 통해 주성분 몇 개로 전체 분산을 설명하려는 시도, 정도로 얘기할 수 있습니다.    

     
예시를 들어서 이해해보죠.


#### 직접 구해보면 이해가 빠릅니다.

가장 전형적인 예인 성적으로 해보겠습니다. 국어, 영어 성적이 다음과 같다고 하겠습니다.
```{r eval=T}
X <- data.frame("국어"=c(95,90,80,60,40,80,95,30,15,60), "영어"=c(95,95,75,70,35,80,90,25,10,70));X
```
산점도를 그려보면,
```{r}
library(ggplot2)
library(magrittr)

 ggplot(X) +
 aes(x = 국어, y = 영어) +
 geom_point(size = 3L, colour = "#1f9e89") +
 labs(x = "국어", y = "영어", title = "국어, 영어 산점도") +
 theme_minimal()
```
    
언어영역 간에는 높은 선형관계가 있음을 확인할 수 있습니다.

표준화를 시켜보면,
```{r}
scale(X) 
```

공분산행렬을 구해보면,
```{r}
X %>% scale() %>% cor()
```

총분산은 2이고, 국어와 영어의 공분산은 약 0.98임을 알 수 있습니다. 꽤나 높은 선형관계가 있음을 알 수 있습니다.    

고윳값을 구하면,
```{r}
X %>% scale() %>% cor() %>% eigen()
```

약 1.98과 0.02임을 알 수 있고, 고유벡터는 (1,1)과 (-1,1)임을 알 수 있습니다.    
    
처음으로 돌아가서, 이 고유벡터들을 주어진 데이터에 표시해봅시다.
```{r}
library(ggplot2)
library(magrittr)

X_scale <- X %>% scale()

 
ggplot(X_scale %>% data.frame) +
 aes(x = 국어, y = 영어) +
 geom_point(size = 3L, colour = "#1f9e89") +
 labs(x = "국어", y = "영어", title = "국어, 영어 산점도") +
 geom_abline(intercept=0, slope=1, color='blue', size=1.5) +
 theme_minimal()
```

우리는 이 파란 직선을 PC1이라고 합시다. 첫번째 주성분이라는 뜻입니다. 이 PC1이 주어진 데이터의 분산을 어느정도 설명하고 있냐면, `r 1.98/2`정도(1.98/2) 설명하고 있습니다. 주어진 총 분산이 2니까요.      
     
고윳값 0.02은 약 1% 밖에 설명해주지 못합니다. 따라서 딱히 쓸 필요가 없다고 생각할 수 있는겁니다.
    
    
- - -


#### 새로운 축(PC1, PC2)으로 바꿔보자.

우리는 더 이상 기존 변수 국어, 영어를 사용할 것이 아니므로 PC1, PC2를 축으로 주어진 데이터를 다시 표현해봅시다.

```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '50%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i13.png")
```

결론적으로 우리는 데이터의 공분산행렬과 고유치들을 통해 두 개의 주성분을 찾아냈었고,
그 중에 하나만의 주성분으로도 충분히 전체 데이터의 99프로를 설명할 수 있었습니다. 

    
     
- - -

## 이게 제일 좋은건가요?

공분산행렬과 고유값, 벡터들을 통해서 구한 주성분이 항상 데이터의 분산을 최대로 설명할 수 있습니다. 이에 대한 수학적 증명도 있지만 생략하겠습니다.

    
    
- - -

## 차원이 더 높아진다고 하면요?

차원이 3차원이라고 가정하면, 주어진 3차원의 데이터를 정사영 시킬 때, 가장 분산을 잘 설명할 수 있는 평면을 찾으면 됩니다. 그 이상의 고차원 데이터도 마찬가지입니다. 몇 개의 주성분을 쓸 지는 데이터 제공자의 사전지식이나 분석자의 역량에 따라 달렸지만 설명도가 90%를 넘긴다던지 하는 특정 값을 사용하기도 합니다.
    
    
     
- - -

## 어디에 쓰이나요?

저희가 일전에 했던 MNIST 데이터도 28 by 28 픽셀로 이루어진 28x28의 차원의 데이터입니다. 이런 데이터에서도 쓰일 수 있습니다. 이미지분석, 주성분회귀 등으로 많이 쓰이고 있습니다.
    
    
- - -

## 그렇다면 모든 데이터에 주성분 분석을 해볼까?

그렇지 않습니다. 여기에 대해서는 발표가 길어져 생략하겠지만,
`Bartlett's test of sphericity`나, `KMO test`, `매니폴드`에 대해서 찾아보시길 바랍니다.


- - -


## 더 공부할 자료

* 적절한 차원 수
* 사이킷런의 PCA 모델
* randomized PCA
* incremental PCA(IPCA)
* 커널 PCA
* 지경 선형 임베딩 (LLE)
* MDS
* t-SNE
* linear discriminant analysis(LDA)


- - -

## 참고자료
`공돌이의 수학노트` blog


- - -

# SVD 특잇값 분해

특잇값 분해(Singular Value Decomposition)는 어떤 행렬을 분해하는 방법 중 하나입니다.    
LU(LR)분해, 스펙트럼 분해, 고윳값 분해, SVD(특잇값 분해) 등이 있습니다.    
기본적으로, 정방행렬이라면 행렬의 대각화를 통해서 행렬의 `곱셈`으로 표현할 수 있습니다.    
대칭이 아닌 일반적인 행렬에 대해선 SVD를 통해 분해가 가능합니다.     
데이터에서 가장 많이 쓰이는 분해는 고윳값 분해와 SVD 분해인 것 같습니다.    
NLP에선, 고차원 벡터들의 가장 중요한 정보를 유지하면서 저차원, 조밀한(dense) 벡터로 압축하고 싶을 때 사용합니다.    
Co-occurence matrix를 SVD하여 word vector를 얻는다던지 합니다.    
특잇값 분해에 대해 논하기 전에, 근본적인 질문부터 던져보겠습니다.        

* 분해해서 무엇이 좋나요?
  * 수치 계산의 기본 부품 쯤 됩니다. 한 번 해놓으면 계산량이 줄어 연산속도가 적은 계산량으로 인해 빨라집니다.
  * 또 머신러닝이나 딥러닝 쪽에서 제법 많이 사용됩니다.
    * 어디에 쓰이나요?
      * 행렬근사, 최소제곱, Rank 계산, 영상이미지압축, Style Transfer, 이미지 인식, PCA 등
      
      
      
- - - 

## 특잇값분해의 정의

특잇값 분해는 임의의 *m* X *n*차원의 행렬 *A*에 대하여 다음과 같이 행렬을 분해할 수 있는 방법 중 하나이다.
$$
A = U\sum V^T
$$
여기서 네 행렬(*A*,*$\sum$*,*U*,*V*)의 크기(혹은 차원)와 성질은 다음과 같습니다.    

* *A* : *m* x *n* 행렬
* *U* : *m* x *m* 직교행렬
* *$\sum$* : *m* x *n* 대각행렬
* *V* : *n* x *n* 직교행렬

직교행렬의 성질에 대해 잠깐 언급하자면,

$$
UU^T = I
$$
$$
U^{-1} = U^T
$$

위와 같습니다.

*$\sum$* 행렬은 mXn 행렬이라면 다음과 같은 행렬입니다.

      
\begin{equation*}
\sum = 
\begin{pmatrix}
\sigma_{1} &      0       &      \cdots     &     0      \\
0          &  \sigma_{2}  &      \cdots     &     0      \\
           &              &      \ddots     &             \\
0          &      0       &      \cdots     & \sigma_{n}  \\
0          &      0       &      \cdots     &      0       \\
\vdots     &    \vdots    &      \vdots     &  \vdots      \\
0          &      0       &      \cdots     &      0
\end{pmatrix}
\end{equation*}


     
     
설명을 기하학적으로 풀어서 쓰면,    
직교하는 벡터 집합에 대하여, 선형변환 후에 그 크기는 변하지만 여전히 직교할 수 있게 만드는 그 직교벡터 집합은 무엇이고, 변형 후의 결과는 무엇인가?    
로 쓸 수 있습니다.

이 식은,
$$
A = U\sum V^T
$$

이렇게 쓸 수 있고,
$$
AV = U\sum
$$
    
이 식을 보면, 직교벡터들의 집합 *V*에 선형변환 *A*를 했을 때, 직교벡터들의 다른 집합 *U*에 크기 변화만 주는 *$\sum$*(scaling)의 곱과 같냐는 말로 해석 가능합니다.    

    
사진으로 나타내면,
```{r, echo=FALSE, fig.cap="SVD", fig.align='center', out.width= '50%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/svd.png")
```

다음과 같습니다.    
    
정리하면, SVD를 이용해 임의의 행렬 A를 정보량(scaling factor)에 따라 여러 layer로 쪼개서 생각할 수 있게 해주는 것입니다.    

다른 기하학적 의미로는, rotation -> scaling -> rotaion 입니다.

```{r, echo=FALSE, fig.cap="", fig.align='center', out.width= '100%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/svd2.png")
```



## SVD in NLP

NLP에서 `co-occurence matrix`에서 `SVD`를 통해 차원이 감소된 단어 벡터를 구하는 과정을 도식화하면 다음과 같다.

```{r, echo=FALSE, fig.cap="", fig.align='center', out.width= '80%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/co-occurence.png")
```
```{r, echo=FALSE, fig.cap="", fig.align='center', out.width= '100%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/SVDreduction.png")
```

## SVD in python
```{python}
import numpy as np

X = np.array([[0,2,2,0,0,0,0,0,0,0], # I
              [2,0,0,1,0,0,0,0,0,0], # ate
              [2,0,0,0,0,0,1,0,1,0], # like
              [0,1,0,0,1,1,0,0,0,0], # a
              [0,0,0,1,0,0,0,0,0,1], # banana
              [0,0,0,1,0,0,0,0,0,1], # cherry
              [0,0,1,0,0,0,0,1,0,0], # deep
              [0,0,0,0,0,0,1,0,0,1], # learning
              [0,0,1,0,0,0,0,0,0,1], # NLP
              [0,0,0,0,1,1,0,1,1,0]]) # . 
              
# X_svd
U,S,V_t = np.linalg.svd(X)
k = 5
theta = sum(S[:k]) / sum(S)
print(f'theta : {theta}')
```

```{python}
U = U[:,:k]
print(U)
```

보다 자세한 내용은    
[공돌이의수학정리노트블로그](https://angeloyeo.github.io/2019/08/01/SVD.html)      
[공돌이의수학정리SVD유튜브](https://www.youtube.com/watch?v=cq5qlYtnLoY&t=1520s)    
를 참고하면 좋습니다.

