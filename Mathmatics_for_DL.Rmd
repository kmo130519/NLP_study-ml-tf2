---
title: "Mathmatics for Deep learning"
output: 
  html_document :
    toc : true
    toc_float: true
header-includes:
    - \usepackage{kotex}
urlcolor : blue
editor_options: 
  chunk_output_type: inline
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 소개

이 파일을 통해 딥러닝과 NLP에 필요한 기본 개념과 여러 모델들을 간단하게나마 수학적으로 풀어나가보고자 한다.

# 주성분분석, 공분산행렬, 고윳값과 고유벡터

## `주성분분석 (PCA)`이 무엇인가요?

주성분분석 (*Principal component analysis*)    
→ 원래 데이터들을 그대로 사용하는 것이 아니라, 원래 데이터들의 `Principal Component` 를 가지고 분석하는 것입니다.    
어려운 말로 쓰자면, 분석에 포함된 변수들의 선형결합을 이용하여 주성분이라 불리는 새로운 변수를 생성하는 방법입니다.    
p개의 변수에는 p개의 주성분이 존재하고, 이 중에 중요한 주성분을 찾아서 그 의미를 해석하게 됩니다.    
변수들의 선형결합으로 이루어진 주성분을 이용하기에 자료가 갖고 있는 대부분의 정보를 활용할 수 있다는 장점이 있습니다.    
예를들어 고등학교에는 여러 교과목이 있지만, 이를 가중평균을 내어 수리영역, 언어영역으로 나누는 것들이 예시입니다.
    
    
- - -


## 그렇다면 `주성분`은 무엇인가요?

Principal Component, 주성분이란 `주어진 데이터(의 분산)을 가장 잘 표현하는 성분`입니다.
```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '100%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i1.gif")
```
    
    
- - -

## 주성분은 `어떻게` 구해요?

주성분은 아무렇게나 구해도 `이론적으론` 상관 없습니다.    

그래도 우리는 데이터 공부하는 사람이니,

* 주어진 데이터의 `정보를 최대한 잃지 않게`
* 주어진 데이터의 `분산을 가장 잘 설명할 수 있게  `  
* 주어진 데이터의 `중심을 지날 수 있게   ` 

그래도 기왕이면 위의 방법들을 거치는 것이 좋지 않을까요?
    
    
- - -

## 그 방법들을 만족하는 주성분을 구하는 `방법`이 있나요?

우선 그 방법을 알기 위해선 간단한 수학 개념을 짚고 가야 합니다.    

* **표준화** (주어진 데이터의 중심을 지날 수 있게, 혹은 단위의 영향이 클까봐(e.g. cm, kg))
* **분산공분산행렬** (주어진 데이터를 가장 잘 설명할 수 있는 하나의 행렬)    
* 그 행렬의 **고윳값과 고유벡터** (주어진 데이터의 분산을 가장 잘 설명할 수 있게)

이 방법들을 만족하는 수학적 방법들을 살펴보겠습니다.
    
    
- - -

### 표준화, 주어진 데이터의 중심을 지나보자.

표준화란, 주어진 데이터의 속성(feature)들을 평균이 0, 분산이 1인 데이터로 변환해주는 과정을 말합니다. 이 과정을 통해야 저희는 모든 속성들을 같은 scale로 볼 수가 있습니다.

$$
z = \frac{x - \mu}{\sigma}
$$
때로는 표준편차로 나눠주지 않고, 평균만 빼주는 경우도 있습니다.



- - -       

### 분산공분산행렬, 주어진 데이터들을 가장 잘 설명하는 행렬

**분산공분산행렬**이란,    
일종의 행렬로써, 데이터의 구조를 설명해주며, 특히 특징 쌍(feature pairs)들의 변동이 얼마나 닮았는가(다른 말로는 얼마만큼이나 함께 변하는가)를 행렬에 나타내고 있습니다.    
보통 기호로는 이렇게 나타냅니다.
$$
\sum
$$

행렬은 **사상(mapping)**입니다.    
쉽게 말하면, 어느 임의의 벡터에 행렬을 곱하면 그 벡터가 바뀐다는 의미입니다.    
기존의 어떤 데이터가 있었다고 하면 우리는 그 **데이터가 어떤 행렬의 곱으로 인해 바뀐 데이터**라고 생각해봅시다.

```{r, echo=F, out.width='50%', fig.ncol=2, fig.align='center'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i3.png")
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i2.png")
```

이번엔 공분산행렬의 값들이 나타내는 건 무엇인지 알아봅시다.
```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '100%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i4.png")
```
```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '100%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i14.png")
```


그렇다면 공분산행렬은 어떻게 구할 수 있을까요?

```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '100%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i10.gif")
```

    
       
- - -

### 그래서요?

결론부터 말씀드리자면, 우리는 데이터의 공분산행렬의 고윳값과 고유벡터를 구할 수만 있다면 데이터의 주성분을 바로 구할 수 있습니다.


- - - 

### 고윳값과 고유벡터는 어떻게 구하는데요?

  
선형대수를 알고계신다면, 고윳값과 고유벡터 관련해서 크기는 변해도 방향은 변하지 않는 값이며, 수식으로는

$$
Ax = \lambda x
$$
로 나타낼 수 있습니다. 이를 만족하는 람다 값이 고윳값이며, 이 람다를 통해서 고유벡터도 구할 수 있습니다.    
즉 어떤 벡터 x에 행렬 A를 곱하고 나서 보니, 그냥 벡터 x에 어떤 상수값 람다를 곱한거나 다름이 없었다는거죠.    
A를 통해서 mapping된 Ax는 사실 x에 상수(람다)만 곱한거와 같으니, 사실은 방향의 변환이 없이 크기만 커진 것과 다름이 없는 겁니다.
    
예시를 들어 설명 드리겠습니다. 다음과 같은 행렬 A, X가 있다고 가정해보죠.  
```{r}
A = matrix(c(2,1,1,2), byrow = T, ncol=2);A
X = matrix(c(1,1), ncol=1);X
```


벡터 X를 좌표에 나타내면 다음과 같습니다.
```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '50%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i11.png")
```

근데 이 벡터 x에 행렬 A를 곱해주면,

```{r}
A%*%X
```

이런 값이 나옵니다. 이 벡터(AX)를 다시 한 번 좌표축에 나타내보면,

```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '50%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i12.png")
```

이렇게 표시할 수 있죠.    

기존의 빨간 (1,1) 벡터에서 (3,3) 벡터로 늘어난 것을 확인할 수 있습니다.     
이럴 거였으면, 벡터 X에 행렬 A를 곱할 필요 없이 그냥 3을 곱해버렸으면 되는 것 아닐까요?    

### 맞습니다.

그게 바로 고윳값의 의미입니다. 다시 한 번 수식과 함께 `고유`라는 것의 의미를 곱씹어보면,
$$
Ax = \lambda x
$$

크기만 변했고 방향은 변하지 않았죠. 이 크기 3을 우리는 고윳값이라고 하며, 이 방향을 고유벡터라고 얘기합니다. 손으로 구하는 방법은 특성방정식과 행렬식에 대해서 알아보시길 권장합니다.     

이름도 어려운 특성방정식과 행렬식을 이용해서 사실 조금 더 엄밀하게 계산해보면 고윳값은 3과 1이 나옵니다. 그리고 이를 통해서 고유벡터도 구해보면,

```{r}
eigen(A)
```

여기서 `$vectors`를 보시면 0.7 비스무리한 숫자로 나와있지만, 엄밀하게 얘기해보면
그냥 (1,1)과 (-1,1)로 생각해도 무관합니다. 



공분산 행렬이 A라고 해봅시다.   
그랬을 때 정리해보면,    
우리는 어떤 데이터를 어떤 행렬을 곱해서 사상된 데이터로 생각해보기로 했습니다. 공분산행렬을 통해 선형변환을 했을 때 그 크기는 변하고 방향이 변하지 않는 벡터가 있다고 하면, 그 벡터의 크기는 각 1배와 3배가 된다는 의미입니다. 그리고 그 방향성은 (1,1)과 (-1,1)이라는거죠.


조금 더 어려운 점은, 이 Matrix의 고유값은 1과 3임을 생각해보면서 따져보면,   

* 이 고윳값의 합은 1 + 3 = 4로, 각 피쳐의 분산의 합과 같습니다.    
* 이 고윳값의 곱은 Matrix의 행렬식과 같습니다.

즉 이 분산공분산행렬과, 고윳값과 고유벡터만 있으면    

* 그 행렬의 고유의 방향성(고유벡터)
* 그 행렬의 고유의 방향성의 크기(고윳값)
* 각 feature의 분산(행렬의 대각원소)
* 각 feature끼리 얼마나 변하는지(행렬의 비대각원소)


까지 다 알 수 있다는 것이죠.


### 아니 그래서요?

주성분분석으로 돌아와보겠습니다. 우리는 그래서 주성분을 어떻게 고를거냐면,

* 분산이 가장 커지는 축을 첫번째 주성분으로 하고,
* 분산이 두번째로 커지는 축을 두번째 주성분으로 하고,
* 분산이 세번째로 커지는 축을 세번째 주성분으로 합니다.
* 조금 더 어려운 말은 각 주성분은 서로간 직교합니다. (공분산행렬의 고유벡터라서..)

이런 식으로 주성분을 선정한다면 우리는 원 데이터를 가장 잘 설명하는 주성분을 선택할 수 있다는겁니다. 조금 어려운 말로 풀어보자면 주성분 분석은 가장 큰 분산을 갖는 부분공간을 보존하는 최적의 선형변환을 통해 주성분 몇 개로 전체 분산을 설명하려는 시도, 정도로 얘기할 수 있습니다.    

     
예시를 들어서 이해해보죠.


#### 직접 구해보면 이해가 빠릅니다.

가장 전형적인 예인 성적으로 해보겠습니다. 국어, 영어 성적이 다음과 같다고 하겠습니다.
```{r eval=T}
X <- data.frame("국어"=c(95,90,80,60,40,80,95,30,15,60), "영어"=c(95,95,75,70,35,80,90,25,10,70));X
```
산점도를 그려보면,
```{r}
library(ggplot2)
library(magrittr)

 ggplot(X) +
 aes(x = 국어, y = 영어) +
 geom_point(size = 3L, colour = "#1f9e89") +
 labs(x = "국어", y = "영어", title = "국어, 영어 산점도") +
 theme_minimal()
```
    
언어영역 간에는 높은 선형관계가 있음을 확인할 수 있습니다.

표준화를 시켜보면,
```{r}
scale(X) 
```

공분산행렬을 구해보면,
```{r}
X %>% scale() %>% cor()
```

총분산은 2이고, 국어와 영어의 공분산은 약 0.98임을 알 수 있습니다. 꽤나 높은 선형관계가 있음을 알 수 있습니다.    

고윳값을 구하면,
```{r}
X %>% scale() %>% cor() %>% eigen()
```

약 1.98과 0.02임을 알 수 있고, 고유벡터는 (1,1)과 (-1,1)임을 알 수 있습니다.    
    
처음으로 돌아가서, 이 고유벡터들을 주어진 데이터에 표시해봅시다.
```{r}
library(ggplot2)
library(magrittr)

X_scale <- X %>% scale()

 
ggplot(X_scale %>% data.frame) +
 aes(x = 국어, y = 영어) +
 geom_point(size = 3L, colour = "#1f9e89") +
 labs(x = "국어", y = "영어", title = "국어, 영어 산점도") +
 geom_abline(intercept=0, slope=1, color='blue', size=1.5) +
 theme_minimal()
```

우리는 이 파란 직선을 PC1이라고 합시다. 첫번째 주성분이라는 뜻입니다. 이 PC1이 주어진 데이터의 분산을 어느정도 설명하고 있냐면, `r 1.98/2`정도(1.98/2) 설명하고 있습니다. 주어진 총 분산이 2니까요.      
     
고윳값 0.02은 약 1% 밖에 설명해주지 못합니다. 따라서 딱히 쓸 필요가 없다고 생각할 수 있는겁니다.
    
    
- - -


#### 새로운 축(PC1, PC2)으로 바꿔보자.

우리는 더 이상 기존 변수 국어, 영어를 사용할 것이 아니므로 PC1, PC2를 축으로 주어진 데이터를 다시 표현해봅시다.

```{r, echo=FALSE, fig.cap="PCA", fig.align='center', out.width= '50%'}

knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/i13.png")
```

결론적으로 우리는 데이터의 공분산행렬과 고유치들을 통해 두 개의 주성분을 찾아냈었고,
그 중에 하나만의 주성분으로도 충분히 전체 데이터의 99프로를 설명할 수 있었습니다. 

    
     
- - -

## 이게 제일 좋은건가요?

공분산행렬과 고유값, 벡터들을 통해서 구한 주성분이 항상 데이터의 분산을 최대로 설명할 수 있습니다. 이에 대한 수학적 증명도 있지만 생략하겠습니다.

    
    
- - -

## 차원이 더 높아진다고 하면요?

차원이 3차원이라고 가정하면, 주어진 3차원의 데이터를 정사영 시킬 때, 가장 분산을 잘 설명할 수 있는 평면을 찾으면 됩니다. 그 이상의 고차원 데이터도 마찬가지입니다. 몇 개의 주성분을 쓸 지는 데이터 제공자의 사전지식이나 분석자의 역량에 따라 달렸지만 설명도가 90%를 넘긴다던지 하는 특정 값을 사용하기도 합니다.
    
    
     
- - -

## 어디에 쓰이나요?

저희가 일전에 했던 MNIST 데이터도 28 by 28 픽셀로 이루어진 28x28의 차원의 데이터입니다. 이런 데이터에서도 쓰일 수 있습니다. 이미지분석, 주성분회귀 등으로 많이 쓰이고 있습니다.
    
    
- - -

## 그렇다면 모든 데이터에 주성분 분석을 해볼까?

그렇지 않습니다. 여기에 대해서는 발표가 길어져 생략하겠지만,
`Bartlett's test of sphericity`나, `KMO test`, `매니폴드`에 대해서 찾아보시길 바랍니다.


- - -


## 더 공부할 자료

* 적절한 차원 수
* 사이킷런의 PCA 모델
* randomized PCA
* incremental PCA(IPCA)
* 커널 PCA
* 지경 선형 임베딩 (LLE)
* MDS
* t-SNE
* linear discriminant analysis(LDA)


- - -

## 참고자료
`공돌이의 수학노트` blog


- - -

# SVD 특잇값 분해

특잇값 분해(Singular Value Decomposition)는 어떤 행렬을 분해하는 방법 중 하나입니다.    
LU(LR)분해, 스펙트럼 분해, 고윳값 분해, SVD(특잇값 분해) 등이 있습니다.    
기본적으로, 정방행렬이라면 행렬의 대각화를 통해서 행렬의 `곱셈`으로 표현할 수 있습니다.    
대칭이 아닌 일반적인 행렬에 대해선 SVD를 통해 분해가 가능합니다.     
데이터에서 가장 많이 쓰이는 분해는 고윳값 분해와 SVD 분해인 것 같습니다.    
NLP에선, 고차원 벡터들의 가장 중요한 정보를 유지하면서 저차원, 조밀한(dense) 벡터로 압축하고 싶을 때 사용합니다.    
Co-occurence matrix를 SVD하여 word vector를 얻는다던지 합니다.    
특잇값 분해에 대해 논하기 전에, 근본적인 질문부터 던져보겠습니다.        

* 분해해서 무엇이 좋나요?
  * 수치 계산의 기본 부품 쯤 됩니다. 한 번 해놓으면 계산량이 줄어 연산속도가 적은 계산량으로 인해 빨라집니다.
  * 또 머신러닝이나 딥러닝 쪽에서 제법 많이 사용됩니다.
    * 어디에 쓰이나요?
      * 행렬근사, 최소제곱, Rank 계산, 영상이미지압축, Style Transfer, 이미지 인식, PCA 등
      
      
      
- - - 

## 특잇값분해의 정의

특잇값 분해는 임의의 *m* X *n*차원의 행렬 *A*에 대하여 다음과 같이 행렬을 분해할 수 있는 방법 중 하나이다.
$$
A = U\sum V^T
$$
여기서 네 행렬(*A*,*$\sum$*,*U*,*V*)의 크기(혹은 차원)와 성질은 다음과 같습니다.    

* *A* : *m* x *n* 행렬
* *U* : *m* x *m* 직교행렬
* *$\sum$* : *m* x *n* 대각행렬
* *V* : *n* x *n* 직교행렬

직교행렬의 성질에 대해 잠깐 언급하자면,

$$
UU^T = I
$$
$$
U^{-1} = U^T
$$

위와 같습니다.

*$\sum$* 행렬은 mXn 행렬이라면 다음과 같은 행렬입니다.

      
\begin{equation*}
\sum = 
\begin{pmatrix}
\sigma_{1} &      0       &      \cdots     &     0      \\
0          &  \sigma_{2}  &      \cdots     &     0      \\
           &              &      \ddots     &             \\
0          &      0       &      \cdots     & \sigma_{n}  \\
0          &      0       &      \cdots     &      0       \\
\vdots     &    \vdots    &      \vdots     &  \vdots      \\
0          &      0       &      \cdots     &      0
\end{pmatrix}
\end{equation*}


     
     
설명을 기하학적으로 풀어서 쓰면,    
직교하는 벡터 집합에 대하여, 선형변환 후에 그 크기는 변하지만 여전히 직교할 수 있게 만드는 그 직교벡터 집합은 무엇이고, 변형 후의 결과는 무엇인가?    
로 쓸 수 있습니다.

이 식은,
$$
A = U\sum V^T
$$

이렇게 쓸 수 있고,
$$
AV = U\sum
$$
    
이 식을 보면, 직교벡터들의 집합 *V*에 선형변환 *A*를 했을 때, 직교벡터들의 다른 집합 *U*에 크기 변화만 주는 *$\sum$*(scaling)의 곱과 같냐는 말로 해석 가능합니다.    

    
사진으로 나타내면,
```{r, echo=FALSE, fig.cap="SVD", fig.align='center', out.width= '50%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/svd.png")
```

다음과 같습니다.    
    
정리하면, SVD를 이용해 임의의 행렬 A를 정보량(scaling factor)에 따라 여러 layer로 쪼개서 생각할 수 있게 해주는 것입니다.    

다른 기하학적 의미로는, rotation -> scaling -> rotaion 입니다.

```{r, echo=FALSE, fig.cap="", fig.align='center', out.width= '100%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/svd2.png")
```



## SVD in NLP

NLP에서 `co-occurence matrix`에서 `SVD`를 통해 차원이 감소된 단어 벡터를 구하는 과정을 도식화하면 다음과 같다.

```{r, echo=FALSE, fig.cap="", fig.align='center', out.width= '80%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/co-occurence.png")
```
```{r, echo=FALSE, fig.cap="", fig.align='center', out.width= '100%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/SVDreduction.png")
```

## SVD in python
```{python}
import numpy as np

X = np.array([[0,2,2,0,0,0,0,0,0,0], # I
              [2,0,0,1,0,0,0,0,0,0], # ate
              [2,0,0,0,0,0,1,0,1,0], # like
              [0,1,0,0,1,1,0,0,0,0], # a
              [0,0,0,1,0,0,0,0,0,1], # banana
              [0,0,0,1,0,0,0,0,0,1], # cherry
              [0,0,1,0,0,0,0,1,0,0], # deep
              [0,0,0,0,0,0,1,0,0,1], # learning
              [0,0,1,0,0,0,0,0,0,1], # NLP
              [0,0,0,0,1,1,0,1,1,0]]) # . 
              
# X_svd
U,S,V_t = np.linalg.svd(X)
k = 5
theta = sum(S[:k]) / sum(S)
print(f'theta : {theta}')
```

```{python}
U = U[:,:k]
print(U)
```

보다 자세한 내용은 [공돌이의수학정리노트블로그](https://angeloyeo.github.io/2019/08/01/SVD.html)나,      
[공돌이의수학정리SVD유튜브](https://www.youtube.com/watch?v=cq5qlYtnLoY&t=1520s)
를 참고하면 좋습니다.




# 경사하강법


* 경사하강법의 직관적 정의    
    
경사하강법(gradient descent)는 함수 값이 낮아지는 방향으로 독립 변수 값을 변형시켜가면서 최종적으로는 최소함수 값을 갖도록 하는 독립변수 값을 찾는 방법.    
    
항상 같이 쓰이는 비유는 앞이 안 보이는 안개가 자욱한 산에서 모든 방향으로 산을 더듬어가며, 산의 높이가 가장 낮아지는 방향을 찾아가며 한발씩 내려가는 과정으로 생각할 수 있다.    

* 경사하강법의 사용 이유

우리는 고등학교 때 local minimum을 찾기 위해서 했던 방법을 되새겨보면, 함수를 독립변수로 미분해서 0이 되는 지점과 이계도함수를 참고해서 찾았다. 그렇다면 이 방법을 쓰면 되지 왜 경사하강법을 사용할까?

   * 함수가 닫힌 형태가 아니거나,
   * 함수가 너무 복잡해 미분계수를 구하기 어렵거나,
   * 경사하강법을 구현하는 게 미분계수를 구하는 것보다 더 쉽거나.
   * 데이터 양이 너무 많은 경우 효율적으로 계산하기 위해
   
경사하강법을 사용한다.


* 경사하강법의 수식 유도

함수의 기울기를 이용해 x의 값을 어디로 옮길 때 최솟값을 갖게 되는지 알아보는 방법.    

  * 기울기가 양수라면 x값이 커지면 함숫값도 커진다.
  * 기울기가 음수라면 x값이 커지면 함숫값은 작아진다.
  
    * 그렇다면 기울기가 양수일 때는 x가 작아지는 방향으로 가고, 기울기가 음수이면 x가 커지는 방향으로 가야한다!
    
    
$$
x_{i+1} = x_{i} - 이동거리 * 기울기의 부호
$$

만약 이동거리를 크게 잡으면 어느 한 점으로 수렴하기가 힘들고, 진동하게 될 가능성이 크다. 따라서 우리는 위 식을,

$$
x_{i+1} = x_{i} - 기울기의 크기 * 기울기의 부호
$$
로 생각하고자 한다. 기울기의 크기가 점점 0에 가까워지면 조심히 이동하잔 뜻이다. 그렇다면 결국 이 식은,

$$
x_{i+1} = x_{i} - 기울기
$$
가 된다.    





## 경사하강법 (딥러닝에서)

딥러닝에서는 사용자의 필요에 맞게 이동거리를 조절할 수 있도록 해주기 위해서 step size 조절 인자를 넣기도 한다. 딥러닝에서의 GD 방식은 대부분 이 step size를 조절할 수 있는 인자를 넣고 있고, 이를 `learning rate`($\eta$)라고 부른다.    

그렇다면, 최종 수식은 다음과 같다.

$$
x_{i+1} = x_{i} - \eta\frac{df}{dx}\bigg|_{x_{i}}
$$

learning rate가 너무 작아도 최솟값까지 수렴하지 못하므로 적절한 값을 설정해야 합니다.    
Local minima 문제로 인해 극솟값에 빠지게 되면 최솟값으로 가지 못하는 경우도 있다.    

지금까지 이해해본 이 경사하강법은 역전파와 같이 사용되게 됩니다. 이해를 위해 딥러닝 모델의 학습과정을 간단하게 정리해보겠습니다.

* 학습하고 싶은 데이터를 넣습니다.
* 사용자가 초기화한 임의의 편향과 가중치를 통해 데이터의 피쳐마다 곱연산을 한 후 합연산을 합니다.
* 그 후 활성화함수에 해당 값을 넣습니다.
* 몇 개의 학습층인지에 따라 위 과정을 거쳐서 계속 나아갑니다.
* 마지막 결과층에서, 우리가 예측한 결과와 실제 결과와의 차이를 계산합니다.
* 예측값과 결과값의 차이를 줄이기 위해 가중치를 다시 설정합니다.

여기서 마지막 과정에서 가중치를 재설정을 어떻게 하는지를 알아야합니다. 가중치를 재설정하려면 경사하강법이 사용됩니다. 가중치에 따른 오차함수를 만들어봅시다. 우리가 함수를 만들 땐 독립변수를 설정합니다. 익숙한 $f(x)$도 x 값에 따라서 결과값이 달라지는 것처럼, 

$$
J(가중치_{i})
$$
이 함수 또한 어떤 가중치에 의해서 오차값이 달라진다는 뜻을 내포하고 있습니다.    
우리가 가중치를 어떻게 설정하고, 어떻게 바꿔나가냐에 따라서 인풋으로 들어간 데이터들의 결과값이 달라질테니까요.    
    
그렇다면 경사하강법으로 돌아와서 이 오차함수 *J*를 어떻게 줄여나갈지 생각해봅시다.    
방법은 위 수학으로 이해한 경사하강법에서와 완전히 같습니다. 다만 한가지 달라지는 점은 우리가 주로 만들 모델들은 단순한 2차원이 아닐 가능성이 높기에 편미분이라는 개념이 도입됩니다.    
    
편미분에 대해 이해하기 위해 먼저 미분의 의미는 $\frac{dy}{dx}$ 에서 이해해보면 x에 의해 종속적인 함숫값 y가 있을 때, x가 아주 조금 움직일 때 y는 얼마만큼 변하니라는 의미이고, $\frac{\triangle y}{\triangle x}$ 이 값의 극한값입니다$\triangle x \rightarrow 0$.    

그럼 편미분으로 돌아와보면, 어느 다변수함수가 있을 때 $f(x,y)$ 편미분 $\frac{\partial f(x,y)}{\partial x}$ 의 의미는, 다변수 함수 $f(x,y)$는 x의 변화에만 대해서는 얼마나 움직이니? 라는 뜻이 됩니다.(y는 신경쓰지 말고! 혹은 고정해놓고!)    

이제 다변수함수가 포함된 경사하강법의 수식을 정리해보겠습니다.    
 
우리는 일변량함수에서 기울기(미분계수)라고 표현했던 것은 직선이었지만, 다변량함수에서는 기울기가 평면이 될 수도 있습니다. 따라서 우리는 앞으로는 *Gradient*라는 단어를 사용할 것입니다.    

수식으로 *gradient*란,
$$
\nabla f = \bigg(\frac{\partial f}{\partial x_{1}}_{,}\frac{\partial f}{\partial x_{2}}_{,}\frac{\partial f}{\partial x_{3}}_{,}\frac{\partial f}{\partial x_{4}}_{,} \bigg)
$$
가 됩니다.    

말로 풀어보면 다변량 함수 f가 있을 때 gradient 행렬의 각 원소는 x1,x2,x3,x4 각각의 영향력만을 생각한(다른 변수의 영향력은 고정한) 방향성입니다.    
조금 더 직관적으로는 가파른 산 위에 있을 때, 각 방향을 90도 씩 네 방향으로 나누면, 각 방향으로서의 기울기 값들을 포함하고 있다는 뜻이죠.    
     
만약 이 *Gradient*에서 $x_{1}$의 영향이 가장 크다면 gradient가 가리키고 있는 벡터의 방향은 x1의 영향을 많이 받겠죠?    

이제 다 왔습니다. 이제 딥러닝에서 흔히 쓰는 기호로만 바꿔주면 됩니다. $x_{i}$ 대신에 가중치인 $w_{i}$를 사용하고, $\nabla f$ 대신에 $\nabla J$를 쓰면 끝입니다.    

$$
w_{i+1} = w_{i} - \eta \nabla J 
$$

조금 수식이 어려워졌으니, 직관적인 이해를 위해 말로 풀겠습니다.    

* 첫 가중치 행렬인 w_1이 있었습니다.
* 딥러닝의 층들을 지나며 합, 곱, 활성화 함수를 거치고 나온 예측값이 결과값과 많이 달라서 가중치를 재조정하고자 합니다.
* 가중치 행렬 안에 있던 값들중 어느 값이 오차에 가장 많은 영향을 줬는지 알아보기 위해 gradient를 구합니다.
* 어느 가중치가 오차에 많은 영향을 줬는지 알았으니 그 가중치를 가장 많이 변화하는 방향으로 내려가야겠으니, learning rate를 곱하고 나서 원래의 가중치 행렬 w_1에서 빼줍니다
* w_2가 구해졌네요.
* 이 과정을 반복합니다.


이렇게 하다보면 우리는 가중치를 가장 적합한 방법으로 선택할 수 있습니다.    
이 과정을 우리는 멋있는 말로 풀면, 역전파와 편미분과 chaing rule을 통해 weight을 loss가 최소화되는 방향으로 조정해나가는 과정으로 얘기할 수 있습니다.


## Optimizer의 종류

영어로 Optimize는 최적화한다는 뜻입니다. 딥러닝에서는 예측값과 실제값의 차이인 loss를 줄여나가자 를 최적화한다고 얘기합니다. 따라서 *Optimizer*로 *Gradient descent*를 줄여나가는 방법을 우리는 알게 된 것입니다. 물론 텐서플로의 케라스를 사용하면 이를 정말 간단한 코드로 짤 수 있습니다.    

물론 이 loss를 계산하는 방법도 다양합니다. 해결하고자 하는 문제와 가장 직결된 것일수도 있으니 이 또한 다양한 방법론들이 존재하고 이를 loss function이라고 합니다. 이는 다음 장에서 자세히 알아보겠습니다.

우리는 Optimizer 중에 가장 근본이 되는 Gradiennt descent를 알았으니 이 친구를 응용하여 개발된 여러가지의 Optimizer들을 살펴보겠습니다.    

먼저 최근까지 개발되온 Optimizer 중 가장 유명한 것들은 다음과 같습니다.

* GD
* SGD
* Momentum
* NAG
* Adagrad
* RMSProp
* AdaDelta
* Adam
* Nadam

그리고 이것들의 흐름은 다음과 같습니다.
```{r, echo=FALSE, fig.cap="", fig.align='center', out.width= '100%'}
knitr::include_graphics("C:/rdata/NLP_study-ml-tf2/typeofOP.png")
```

개인적으로 느끼기엔, adam function이 가장 흔하게 사용되는 것 같습니다.
 